{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTYSc7pO8qEL",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow supports  two types of api\n",
        "1.Low level api(used to build models from scratch)\n",
        "2.High level api(complex operations can be done easily)\n",
        "It is available in the form of estimators and keras, with estimators we can use all predefined machine learning models.\n",
        "\n",
        "Following are different types of neural networks.\n",
        "1. Artificial Neural Networks[ANN]\n",
        "  a.Simple Neural Networks---> have only one hidden layer along with one input layer and one output layer.\n",
        "  b.Deep Neural Networks---> have multiple hidden layers with one input layer and one output layer.\n",
        "2. Convolutional Neural Networks[CNN]\n",
        "This is used for computer vision, especially for image objects.\n",
        "3. Recurrent Neural Networks[RNN]\n",
        "this is used to predict next moment of an object\n",
        "4. LongShortTermMemory[extension to RNN]\n",
        "this is used to overcome the problems of RNN.\n",
        "\n",
        "Complex networks can easily be designed in Tensorflow with Keras.\n",
        "Before tensorflow 2.0 tensorflow and keras are separate libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1pH24th3ev3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fcb71709-a32b-4563-9a45-50bdef110c09"
      },
      "source": [
        "#Sample code using tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "mnist=tf.keras.datasets.mnist\n",
        "#Tensorflow offers many datasets with features and labelsto build and train models\n",
        "#Here we imported opensource data for handwritten digit images with their labels\n",
        "#labels(10 classifiers with values from 0 to 9)\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "#a sample function to understand the above statement\n",
        "\"\"\" def samp(u,v):\n",
        "      a=u+v\n",
        "      b=u-v\n",
        "      return (a,b)\n",
        " #way1\n",
        " A=samp(10,20)     \n",
        " A[0]=30\n",
        " A[1]=-10\n",
        " #way2\n",
        " u,v=samp(10,20)\"\"\"\n",
        " #mnist has a predefined method load_data(), which returns a tuple object.\n",
        " #Tuple has two elements, each element is a tuple((),()), each subtuple has two elements(features and labels)\n",
        " #The first sub tuple is for train set(x_train,y_train)--->80% data, second sub tuple (x_test,y_test) is for test set(20% data)\n",
        "x_train,x_test=x_train/255.0,x_test/255.0    \n",
        "#In order to avoid weightage issues for the models that use derivatives, we use scaling techniques.\n",
        "#Each image pixel are in between 0 and 255, if image is 30*30, 900 pixels are available\n",
        "############################\n",
        "\"\"\"\n",
        "To understand the above code we take this code\n",
        "a =100\n",
        "b=200\n",
        "a,b=a/10,b/20\n",
        "\n",
        "When naivebayes, decisiontree and randomforest is used no need to scale the features.\n",
        "Two types of scaling techniques\n",
        "1.MinMaxScaling technique(x/max(x)) \n",
        "2.Normalized scaling technique([x-max(x)]/sd(x))\n",
        "\n",
        "Second technique has more benefits yet we choose the first technique as we are dealing with the images, images should not have negitive values.\n",
        "When 900 pixels in the above code is flattened 900 clumns will be produced, across these columns there will be weightage diference due to unscaled data.\n",
        "If data is not scaled the derivatives can't reduce the lsos during training, for all the models which use derivatives input data if necessary output lables to be scaled.\n",
        "We didn't scale the output lables as the problem is classification problem but not regression one.\n",
        "Note: if problem is classification, target label should not be scaled.\n",
        "\"\"\"\n",
        "#############################\n",
        "\n",
        "model= tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                   tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                   tf.keras.layers.Dropout(0.2),\n",
        "                                   tf.keras.layers.Dense(10,activation='softmax')])\n",
        "#########################################\n",
        "\"\"\"\"\n",
        "tf.keras.layers.Flatten(input_shape=(28,28))----> grey scale will have only 2 dimensions, color images have 3 dimensional(column,row,color)\n",
        "Here input is grey scale image as per mnist dataset its a two dimensional array.eg(30,30)---> 900 pixels\n",
        "\n",
        "If input is color image its a three dimensional array, if keras running on tensorflow the shape should be (col,row,color)\n",
        "if keras is on theano then the shape should be (color,row,col)\n",
        "\n",
        "[[1 2\n",
        "3 4]] ---> flatteen---> [1 2 3 4]\n",
        "when you flatten this multi dimensional array will be converted into single dimensional array, if trainset has 8000 images with 30X30 image size then input matrix should be 8000X900.\n",
        "\n",
        "tf.keras.layers.Dense(128,activation='relu')----> Dense will construct layers along with weights(random weights).\n",
        "The first dense takes input from previous step i.e., flatten step, each flattened image has 900 pixels, input layer will be initialized with 900 neurons.\n",
        "It initializes 128 neurons in hidden layer, so weight matrix should be 900X128 with random values.\n",
        "At hidden layer activation function should be executed following are different activation functions.\n",
        "1.sigmoid\n",
        "2.relu\n",
        "3.softmax\n",
        "Dense pushes,dot product of input matrix and weight matrix to first hidden layer\n",
        "input--->800*900\n",
        "w1---->900*128\n",
        "p=x.dot(w1)--->8000*128\n",
        "At hidden layer 1---> relu(p) ---> 8000*128\n",
        "but this will be happening during training the network.\n",
        "\n",
        "tf.keras.layers.Dropout(0.2)----> Dropout will replace 20% of each image with zeros, this is called masking.\n",
        "Masking has an advantage that during predictions eventhough some of the features got missed the model still able to recognize the right classifier of the image.\n",
        "\n",
        "tf.keras.layers.Dense(10,activation='softmax')--------> it's a second dense takes input from dropped content(masking) and initiates 10 neurons in next layer.\n",
        "It is  the last dense hence the next layer will be output layer.\n",
        "\n",
        "between hidden layer and output layer weight matrix will be initialized\n",
        "w2--->128*10 \n",
        "\n",
        "What is the input for output layer?\n",
        "dot product of output of hidden layer and w2\n",
        "H=relu(x.dot(w1))--->8000*128\n",
        "At output layer activation should be executed\n",
        "softmax(H.dot(w2))--->8000*10\n",
        "H---->8000*128\n",
        "w2--->128*10\n",
        "\n",
        "\"\"\"\n",
        "#########################################\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\\ntf.keras.layers.Flatten(input_shape=(28,28))----> grey scale will have only 2 dimensions, color images have 3 dimensional(column,row,color)\\nHere input is grey scale image as per mnist dataset its a two dimensional array.eg(30,30)---> 900 pixels\\n\\nIf input is color image its a three dimensional array, if keras running on tensorflow the shape should be (col,row,color)\\nif keras is on theano then the shape should be (color,row,col)\\n\\n[[1 2\\n3 4]] ---> flatteen---> [1 2 3 4]\\nwhen you flatten this multi dimensional array will be converted into single dimensional array, if trainset has 8000 images with 30X30 image size then input matrix should be 8000X900.\\n\\ntf.keras.layers.Dense(128,activation=\\'relu\\')----> Dense will construct layers along with weights(random weights).\\nThe first dense takes input from previous step i.e., flatten step, each flattened image has 900 pixels, input layer will be initialized with 900 neurons.\\nIt initializes 128 neurons in hidden layer, so weight matrix should be 900X128 with random values.\\nAt hidden layer activation function should be executed following are different activation functions.\\n1.sigmoid\\n2.relu\\n3.softmax\\nDense pushes,dot product of input matrix and weight matrix to first hidden layer\\ninput--->800*900\\nw1---->900*128\\np=x.dot(w1)--->8000*128\\nAt hidden layer 1---> relu(p) ---> 8000*128\\nbut this will be happening during training the network.\\n\\ntf.keras.layers.Dropout(0.2)----> Dropout will replace 20% of each image with zeros, this is called masking.\\nMasking has an advantage that during predictions eventhough some of the features got missed the model still able to recognize the right classifier of the image.\\n\\ntf.keras.layers.Dense(10,activation=\\'softmax\\')--------> it\\'s a second dense takes input from dropped content(masking) and initiates 10 neurons in next layer.\\nIt is  the last dense hence the next layer will be output layer.\\n\\nbetween hidden layer and output layer weight matrix will be initialized\\nw2--->128*10 \\n\\nWhat is the input for output layer?\\ndot product of output of hidden layer and w2\\nH=relu(x.dot(w1))--->8000*128\\nAt output layer activation should be executed\\nsoftmax(H.dot(w2))--->8000*10\\nH---->8000*128\\nw2--->128*10\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3vCda0Wf6J5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cb3ff0c8-7779-4b5d-a004-80b8aac1cc38"
      },
      "source": [
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "###############################\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "###############################\n",
        "model.fit(x_train,y_train,epochs=5)\n",
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.2982 - acc: 0.9125\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.1413 - acc: 0.9575\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.1064 - acc: 0.9671\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0856 - acc: 0.9737\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0741 - acc: 0.9772\n",
            "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0720 - acc: 0.9783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07199583536460996, 0.9783]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}